  // if (response.body) response.body.pipeTo(res.body);

  // response.body.on("data", (chunk) => {
  //   res.write(chunk, "binary");
  // });
  //
  // response.body.on("end", () => {
  //   res.end();
  // });

  // const stream = fetch(backend, req);
  // console.log("upload", stream);
  // // stream
  // return new NextResponse();

  // const stream = iteratorToStream(iterator)

  // try {
  //   const resp = await fetch(backend, req);
  //   console.log("resp", resp);
  //   return NextResponse.json({ ok: true }, { status: 200 });
  // } catch (e: any) {
  //   console.log("error", e);
  //   return NextResponse.json({ error: e.message }, { status: 500 });
  // }


# import re
# from bs4 import BeautifulSoup  # , SoupStrainer
# from parser import langchain_docs_extractor
# from langchain.document_loaders import RecursiveUrlLoader, SitemapLoader
# from langchain.utils.html import PREFIXES_TO_IGNORE_REGEX, SUFFIXES_TO_IGNORE_REGEX


# def simple_extractor(html: str) -> str:
#     soup = BeautifulSoup(html, "lxml")
#     return re.sub(r"\n\n+", "\n\n", soup.text).strip()


# def metadata_extractor(meta: dict, soup: BeautifulSoup) -> dict:
#     title = soup.find("title")
#     description = soup.find("meta", attrs={"name": "description"})
#     html = soup.find("html")
#     return {
#         "source": meta["loc"],
#         "title": title.get_text() if title else "",
#         "description": description.get("content", "") if description else "",
#         "language": html.get("lang", "") if html else "",
#         **meta,
#     }


# def load_langchain_docs():
#     return SitemapLoader(
#         "https://python.langchain.com/sitemap.xml",
#         filter_urls=["https://python.langchain.com/"],
#         parsing_function=langchain_docs_extractor,
#         default_parser="lxml",
#         bs_kwargs={
#             "parse_only": SoupStrainer(
#                 name=("article", "title", "html", "lang", "content")
#             ),
#         },
#         meta_function=metadata_extractor,
#     ).load()
#
#
# def load_langsmith_docs():
#     return RecursiveUrlLoader(
#         url="https://docs.smith.langchain.com/",
#         max_depth=8,
#         extractor=simple_extractor,
#         prevent_outside=True,
#         use_async=True,
#         timeout=600,
#         # Drop trailing / to avoid duplicate pages.
#         link_regex=(
#             f"href=[\"']{PREFIXES_TO_IGNORE_REGEX}((?:{SUFFIXES_TO_IGNORE_REGEX}.)*?)"
#             r"(?:[\#'\"]|\/[\#'\"])"
#         ),
#         check_response_status=True,
#     ).load()
#
#
# def load_api_docs():
#     return RecursiveUrlLoader(
#         url="https://api.python.langchain.com/en/latest/",
#         max_depth=8,
#         extractor=simple_extractor,
#         prevent_outside=True,
#         use_async=True,
#         timeout=600,
#         # Drop trailing / to avoid duplicate pages.
#         link_regex=(
#             f"href=[\"']{PREFIXES_TO_IGNORE_REGEX}((?:{SUFFIXES_TO_IGNORE_REGEX}.)*?)"
#             r"(?:[\#'\"]|\/[\#'\"])"
#         ),
#         check_response_status=True,
#         exclude_dirs=(
#             "https://api.python.langchain.com/en/latest/_sources",
#             "https://api.python.langchain.com/en/latest/_modules",
#         ),
#     ).load()
# from pydantic import BaseModel
# import asyncio
# import langsmith
# from typing import Optional, Union
# from uuid import UUID

# class SendFeedbackBody(BaseModel):
#     run_id: UUID
#     key: str = "user_score"
#
#     score: Union[float, int, bool, None] = None
#     feedback_id: Optional[UUID] = None
#     comment: Optional[str] = None
#
#
# @app.post("/feedback")
# async def send_feedback(body: SendFeedbackBody):
#     client.create_feedback(
#         body.run_id,
#         body.key,
#         score=body.score,
#         comment=body.comment,
#         feedback_id=body.feedback_id,
#     )
#     return {"result": "posted feedback successfully", "code": 200}
#
#
# class UpdateFeedbackBody(BaseModel):
#     feedback_id: UUID
#     score: Union[float, int, bool, None] = None
#     comment: Optional[str] = None
#
#
# @app.patch("/feedback")
# async def update_feedback(body: UpdateFeedbackBody):
#     feedback_id = body.feedback_id
#     if feedback_id is None:
#         return {
#             "result": "No feedback ID provided",
#             "code": 400,
#         }
#     client.update_feedback(
#         feedback_id,
#         score=body.score,
#         comment=body.comment,
#     )
#     return {"result": "patched feedback successfully", "code": 200}
#
#
# # TODO: Update when async API is available
# async def _arun(func, *args, **kwargs):
#     return await asyncio.get_running_loop().run_in_executor(None, func, *args, **kwargs)
#
#
# async def aget_trace_url(run_id: str) -> str:
#     for i in range(5):
#         try:
#             await _arun(client.read_run, run_id)
#             break
#         except langsmith.utils.LangSmithError:
#             await asyncio.sleep(1**i)
#
#     if await _arun(client.run_is_shared, run_id):
#         return await _arun(client.read_run_shared_link, run_id)
#     return await _arun(client.share_run, run_id)
#
#
# class GetTraceBody(BaseModel):
#     run_id: UUID
#
#
# @app.post("/get_trace")
# async def get_trace(body: GetTraceBody):
#     run_id = body.run_id
#     if run_id is None:
#         return {
#             "result": "No LangSmith run ID provided",
#             "code": 400,
#         }
#     return await aget_trace_url(str(run_id))



